{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Processing - Instituto Superior Técnico\n",
    "### Laboratory Assignment 2 - Spoken Language Indentification challenge\n",
    "\n",
    "The second laboratory assignment of the course is designed to simulate a **spoken language identification** challenge. In this challenge, partipants (a.k.a students enrolled in the course) receive a train, development and evaluation (blind) data set, and a simple (and weak) baseline system for the task at hand: closed-set identification of the spoken language in a given audio file out of a set of six target langauges: Basque,  Catalan,  English,  Galician,  Portuguese and  Spanish.\n",
    "\n",
    "The **goal** for each participant is to develop/build the best spoken language identification system. To this end, participants are encouraged to modify this baseline, incorporate any other techniques and in general explore any approach that permit  improving their results.\n",
    "\n",
    "During the first week (Part 1), students are expected to:\n",
    "- Run and understand the main components of the baseline.\n",
    "- Propose and develop simple modifications to the baseline feature extraction process.\n",
    "- Propose and develop simple modifications to the baseline GMM language models.\n",
    "- Evaluate the models on the development partition.\n",
    "\n",
    "During the second week (Part 2), students are expected to:\n",
    "- Propose and develop other *classical* modifications to any component of the processing pipeline (openSMILE features, segment-based features, SVM classifiers, MLP/CNN classifiers, etc.)\n",
    "- Run and understand the second part of Notebook that explores a pre-trained model.\n",
    "- Propose and develop more recent advanced approaches, including x-vectors.\n",
    "- Evaluate the models on the development partition.\n",
    "- Obtain predictions for the blind test partition and prepare the submission.\n",
    "\n",
    "The challenge distinguishes two different tracks or evaluation conditions:\n",
    "- Track 1 - Participants are not allowed to use any kind of pre-trained model (such as x-vectors).\n",
    "- Track 2 - Participants are allowed to use anything.\n",
    "\n",
    "\n",
    "## About the data\n",
    "\n",
    "The data consists of mono audio files sampled at 16 kHz all of them containing speech of only one of the following target languages:\n",
    "```python \n",
    "LANGUAGES = ('Basque',  'Catalan',  'English',  'Galician',  'Portuguese',  'Spanish')\n",
    "```\n",
    "\n",
    "The dataset is organized in 4 partitions:\n",
    "- `'train'`: This is the full training set, consisting of 3060 clean audio samples correponding to speech segments of TV broadcast shows. (**ATENTION**: Do not use this dataset for training your models, unless your system is very fast or if you want to build your final model. It can be slow)\n",
    "- `'train100'`: This is a subset of the full training set that consists of 100 audio files per target language (**RECOMMENDATION**: Use this partition in your quick experiments, to more rapidly validate alternatives)\n",
    "- `'dev'`: This is the development set. It contains audio extracted from YouTube. You will typically use this to validate the quality of your model.\n",
    "- `'evl'`: This is the evaluation set. It contains audio extracted from YouTube. You don't have the groud-truth for this set. You are expected to produce it and submit it.\n",
    "\n",
    "The data used in this challenge is a subset of the KALAKA-3 database: https://aclanthology.org/L14-1576/\n",
    "\n",
    "The  difference is that only the clean train audio segments and the Plenty Closed evaluation condition have been considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting\n",
    "The following conditions are neecessary to run correctly this notebook:\n",
    "\n",
    "*   All modules included in the requirements file need to be \n",
    "installed in the Python environment.\n",
    "*   The module `pf_tools` needs to be accessible (if you are using Google Colab, you will need to copy the `pf_tools.py` every time you start a new session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pf_tools import CheckThisCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can you download (and process) the data\n",
    "\n",
    "The first thing we have to do is to set our working directory. If you are using Google Colab, you  probably want to mount Google Drive to keep persistent information, such as data, features and models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/duartealmeida/PF/Spoken-Language-Processing/lab02'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "CWD = os.getcwd() # Change this variable to your working directory to store data, features and models\n",
    "CWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Kalaka` permits downloading, transforming and storing the different data partitions. Each `Kalaka` instance can be used to iterate over all the samples of the partition. It can also be used in combination with pytorch dataloader to read batches of data to train neural networks with pytorch. For instance, consider the following piece of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duartealmeida/PF/Spoken-Language-Processing/lab02/pf_tools.py:91: UserWarning: The feature directory already exists, and no new feature extraction will be performed.\n",
      "  warnings.warn(\"The feature directory already exists, and no new feature extraction will be performed.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pf_tools import Kalaka\n",
    "import librosa \n",
    "\n",
    "def audio_transform(filename):\n",
    "    y, _ = librosa.load(filename, sr=16000, mono=True)\n",
    "    return y.reshape(-1,1)\n",
    "    \n",
    "trainkalaka = Kalaka(CWD, 'train100', transform_id='raw', audio_transform=audio_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This will first download and uncompress the .tar.gz file containing all the necessary data of the `'train100'` partition, that is, the audio files that are stored to disk (in CWD/train100/audio/) and key file (CWD/train100/key.lst). Then, the audio transformation `'transform'` will be applied to each file and the result stored to disk CWD/train100/raw/. \n",
    "\n",
    "**Audio transformations** receive a filename of an audio file and returns an array of dimensions (NxD), in which N is the time dimension and D the dimension of the feature vector. In this simple case D is 1 because the transform is just returning the raw audio signal.\n",
    "\n",
    "The `Kalaka` class permits chunking the output of the audio transformation (of size NxD) in chunks of CxD size. The chunking operation divides the orignal sample, in multiple smaller samples with a configurable chunk size and hop length. These chunks can be further transformed and stored as individual feature files. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainkalaka = Kalaka(CWD, 'train100', \n",
    "                     transform_id='chunks', \n",
    "                     audio_transform=audio_transform, \n",
    "                     chunk_size=4*16000, \n",
    "                     chunk_hop=2*16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download and uncompress the partition data, only if was not already done before. Then, as previously, the simple tranform that returns the waveform is applied to each audio file. After this, the resulting array of dimension Nx1, in which N=16000xduration_in_seconds, is split in continuous chunks of length 64000 (that is, 4 seconds) with chunk hop of 2 seconds. Each one of these chunks of 4 seconds is stored and will be accessed whenever we iterate the dataset. \n",
    "\n",
    "Adittionally, the optional argument `chunk_transform` pertmits defining a transformation to be applied to each chunk before storing to disk. It can be any function that receives an array of size CxD and returns an array HxW, in which H is the *new time dimension*. For instance, the following example takes the audio segments of 64000x1, computes the mean and variance every 0.1 sec (1600 samples) and returns a feature vector of size 40x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transform(x):\n",
    "    x = x.reshape(-1,1600)\n",
    "    return np.concatenate((x.mean(axis=1, keepdims=True), x.std(axis=1, keepdims=True)),axis=1)\n",
    "\n",
    "trainkalaka = Kalaka(CWD, 'train100', \n",
    "                     transform_id='chunks_mv', \n",
    "                     audio_transform=audio_transform, \n",
    "                     chunk_size=4*16000, \n",
    "                     chunk_hop=2*16000, \n",
    "                     chunk_transform=chunk_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice that, while the above example is probably useless as an effective feature extraction method, the proper combination of audio and chunk transformations is expected to permit quite flexible feature extraction that (hopefully) can match the needs of almost any training setting. \n",
    "\n",
    "Once we have instanciated a Kalaka dataset, it can be iterated to have access to each processed sample, for instance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (40, 2) 6 0006ebda\n",
      "1000 (40, 2) 1 11a3790f\n",
      "2000 (40, 2) 4 271621da\n",
      "3000 (40, 2) 5 3b9d2c2f\n",
      "4000 (40, 2) 1 4d10f8f2\n",
      "5000 (40, 2) 6 5fb949a1\n",
      "6000 (40, 2) 6 70d28e30\n",
      "7000 (40, 2) 6 7d5515f0\n",
      "8000 (40, 2) 3 937623c2\n",
      "9000 (40, 2) 5 a4a7491b\n",
      "10000 (40, 2) 5 bc3f6e51\n",
      "11000 (40, 2) 3 cd82bcca\n",
      "12000 (40, 2) 1 df0b27f2\n",
      "13000 (40, 2) 3 f3ba1f4d\n",
      "Finished reading all data in 3.683227300643921\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i, sample in enumerate(trainkalaka):\n",
    "    data, label, basename = sample # array, int, str\n",
    "    if i % 1000 == 0:\n",
    "        print(i, data.shape, label, basename)\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the `Kalaka` class to check the  number of files and size (in minutes) of the training set for each target language. You cankKeep these numbers to include in your system description paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                     | 0/600 [00:00<?, ?it/s]/Users/duartealmeida/PF/Spoken-Language-Processing/lab02/pf_tools.py:145: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  minutes[language] += librosa.get_duration(filename=audioin)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:18<00:00, 32.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files per language:\n",
      "\tBasque: 100\n",
      "\tCatalan: 100\n",
      "\tEnglish: 100\n",
      "\tGalician: 100\n",
      "\tPortuguese: 100\n",
      "\tSpanish: 100\n",
      "\n",
      "Minutes per language:\n",
      "\tBasque: 4743.38\n",
      "\tCatalan: 5111.188999999999\n",
      "\tEnglish: 5254.2998125\n",
      "\tGalician: 4833.832437500003\n",
      "\tPortuguese: 4569.997062499999\n",
      "\tSpanish: 4745.947999999998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the training data to find the size of each training language\n",
    "num_files, minutes = trainkalaka.get_metrics_per_language()\n",
    "\n",
    "print(\"Number of files per language:\\n\\tBasque: {}\\n\\tCatalan: {}\\n\\tEnglish: {}\\n\\tGalician: {}\\n\\tPortuguese: {}\\n\\tSpanish: {}\\n\"\\\n",
    "    .format(num_files['Basque'], num_files['Catalan'], num_files['English'], num_files['Galician'], num_files['Portuguese'], num_files['Spanish']))\n",
    "print(\"Minutes per language:\\n\\tBasque: {}\\n\\tCatalan: {}\\n\\tEnglish: {}\\n\\tGalician: {}\\n\\tPortuguese: {}\\n\\tSpanish: {}\\n\"\\\n",
    "    .format(minutes['Basque'], minutes['Catalan'], minutes['English'], minutes['Galician'], minutes['Portuguese'], minutes['Spanish']))\n",
    "\n",
    "#raise CheckThisCell ## <---- Remove this after completeing/checking this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `Kalaka` class extends the `torch.utils.data.Dataset` and it can be used in combination with a Pytorch DataLoader to read data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duartealmeida/PF/Spoken-Language-Processing/lab02/pf_tools.py:91: UserWarning: The feature directory already exists, and no new feature extraction will be performed.\n",
      "  warnings.warn(\"The feature directory already exists, and no new feature extraction will be performed.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "torch.Size([10, 40, 2]) torch.Size([10]) 10\n",
      "Finished reading all data in 1.649137020111084\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "trainkalaka = Kalaka(CWD, 'train100', \n",
    "                     transform_id='chunks_mv', \n",
    "                     audio_transform=audio_transform, \n",
    "                     chunk_size=4*16000, \n",
    "                     chunk_hop=2*16000, \n",
    "                     chunk_transform=chunk_transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=trainkalaka,\n",
    "        batch_size=10,\n",
    "        shuffle=True\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "for i, batch in enumerate(dataloader):\n",
    "    data, label, basename = batch\n",
    "    if i % 100 == 0:\n",
    "        print(data.shape, label.shape, len(basename))\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remind that you can change anything you want. This includes the Kalaka class. Eventually, you can decide not using it at all and loading the data in some alternative way.  You can inspect the class to find the URLs for downloading the datasets. It is up to you! \n",
    "\n",
    "Before moving to the next stage, you probably want to delete the folders containing the dummy features that you just generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - The baseline (Track 1)\n",
    "The baseline consists of MFCC feature extraction  (based on the `librosa` module) with SDC computation and VAD removal, followed by GMMs of 64 dimensions for each language (using the `sklearn` module). The rest of this notebook contains the guide and code cells (some of them partially incomplete) that permit implementing this baseline and score it on the development set. Read carefully the Markdown information, but also the comments inside the code cells (they provide useful information and hints), and also the code itself. The better you understand it, the easier will be modyfing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pf_tools import Kalaka\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import csv \n",
    "from tqdm import tqdm\n",
    "\n",
    "from nn import FeedforwardNetwork, ClassificationDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from parselmouth import praat\n",
    "\n",
    "GLOBAL_SEED = 35731\n",
    "\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "\n",
    "\n",
    "LANGUAGES = ('Basque',  'Catalan',  'English',  'Galician',  'Portuguese',  'Spanish')\n",
    "LANG2ID = {'Basque':1, 'Catalan':2, 'English':3, 'Galician':4, 'Portuguese':5, 'Spanish':6}\n",
    "ID2LANG = dict((LANG2ID[k],k)for k in LANG2ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The baseline feature extraction module\n",
    "The next function extracts MFCCs, but there are plenty of things that can be improved. You are free to change anything you want, including the number of formal parameters, the number of returned expressions, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Read carefully this function and understand it\n",
    "def feat_extract(filename, orig_sr=16000, mono=True, n_mfcc = 13, remove_c0=False, delta_order=0, apply_sdc=False, apply_vad=False, apply_cmvn=False,\n",
    "                 num_formants = 0, cluster_order = 0):\n",
    "    \n",
    "    sr=16000\n",
    "    n_mels = 40\n",
    "    n_fft = 512 \n",
    "    hop_length = 160\n",
    "    fmin = 50\n",
    "    fmax = 7800\n",
    "    \n",
    "    if apply_sdc and (delta_order > 0):\n",
    "        raise ValueError(\"Applying SDC and delta > 0 is not compatible\")\n",
    "\n",
    "    # Load audio wav into numpy array\n",
    "    y, _ = librosa.load(filename, sr=orig_sr, mono=mono)\n",
    "    \n",
    "    # Resample in case it's needed\n",
    "    if orig_sr != sr:\n",
    "        y = librosa.resample(y, orig_sr=orig_sr,target_sr=sr)\n",
    "\n",
    "    ## OPTIONAL ADDIDITIONAL STAGES - LAB WORK\n",
    "    # 1 - PREPROCESSING - Typical preprocessing may include normalization of audio (mean removal), \n",
    "    #                       but also speech enhancement and others more complex\n",
    "\n",
    "    # Extract MFFCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_fft=n_fft, \n",
    "                                n_mfcc=n_mfcc, n_mels=n_mels, \n",
    "                                hop_length=hop_length, \n",
    "                                fmin=fmin, fmax=fmax, htk=False).T\n",
    "    \n",
    "    ## OPTIONAL ADDIDITIONAL STAGES - LAB WORK\n",
    "\n",
    "    # 2 Compute deltas --> Hint you can use librosa (order size may be a parameter?)\n",
    "    if delta_order > 0:\n",
    "        mfcc = compute_delta(mfcc)\n",
    "    \n",
    "    # 3 SDC --> Hint: You can build this using deltas (of extented context)  \n",
    "    if apply_sdc:\n",
    "        mfcc = compute_sdc(mfcc)\n",
    "    \n",
    "    # Compute formants\n",
    "    if num_formants > 0:\n",
    "        sound = parselmouth.Sound(filename)\n",
    "        formants = praat.call(sound, \"To Formant (burg)\", (hop_length / sr), num_formants, 5500, (n_fft / sr), 50)\n",
    "        all_formants = np.zeros((len(mfcc), num_formants))\n",
    "        for i in range(len(mfcc)):\n",
    "            t = i * (hop_length / sr)\n",
    "            for j in range(num_formants):\n",
    "                all_formants[i, j] = praat.call(formants, \"Get value at time\", j + 1, t, \"Hertz\", \"Linear\")\n",
    "        mfcc = np.concatenate((mfcc, all_formants), axis = 1)\n",
    "        mfcc = np.nan_to_num(mfcc)\n",
    "        \n",
    "    \n",
    "    # 4 COMPUTE VAD --> Hint: You can use any vad (theshold energy, something avaialble in the net, a biGaussian model...).\n",
    "    #                         Coeff0 is highly related with Energy and sometimes it is removed\n",
    "    #                   ATTENTION: Using a VAD may have a significant impact  \n",
    "    if apply_vad:\n",
    "        energy = librosa.feature.rms(y = y, hop_length = hop_length)\n",
    "        mfcc, vad = compute_vad(mfcc, energy = librosa.feature.rms(y = y, hop_length = hop_length).reshape(-1, 1))\n",
    "        mfcc = mfcc[:, 1:]\n",
    "             \n",
    "    # 5 APPLY CMVN --> ATTENTION: Using normalization may have a significant impact\n",
    "    if apply_cmvn:\n",
    "        mfcc = compute_cmvn(mfcc)\n",
    "        \n",
    "    if cluster_order > 0:\n",
    "        mfcc = KMeans(cluster_order).fit(mfcc).cluster_centers_\n",
    "    #print(mfcc)\n",
    "        \n",
    "    \n",
    "    return mfcc, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to define some or all of the following steps to improve your feature extraction pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays have methods to compute mean and variance, so this one should be really easy\n",
    "def compute_cmvn(features):\n",
    "    # subtract the mean and divide by the variance along each column\n",
    "    if (features.shape[0] == 0):\n",
    "        return features\n",
    "    else:\n",
    "        stddev = np.std(features, axis = 0)\n",
    "        stddev[stddev == 0] = 1\n",
    "        return (features - np.mean(features, axis = 0)) / stddev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librosa contains functions to compute deltas\n",
    "def compute_delta(features, win=3, delta_order=2, keep_static=True):\n",
    "    deltas = librosa.feature.delta(features, width = win, order = 1)\n",
    "    for i in range(1, delta_order):\n",
    "        new_delta = librosa.feature.delta(features, width = win, order = i + 1)\n",
    "        deltas = np.concatenate((deltas, new_delta), axis = 1)\n",
    "    if keep_static:\n",
    "        return np.concatenate((features, deltas), axis = 1)\n",
    "    else:\n",
    "        return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute deltas and then select previous and next deltas with fixed intervals to cocatentate. \n",
    "# You may need to code a bit here or dinf some function that helps with this\n",
    "def compute_sdc(features, P=3, D=1, K=7, keep_static=True):\n",
    "    N = len(features)\n",
    "    \n",
    "    # need to pad with D zeros before and after (to allow order-1 delta at all vectors)\n",
    "    # need to pad with (K - 1) * P zeros at the end so that the last vector can have the other K - 1 deltas\n",
    "    features_pad = np.pad(features, ((D, D + ((K - 1) * P)), (0, 0)), mode = \"edge\")\n",
    "    \n",
    "    # NOTE: librosa does not allow deltas at a window smaller than 3\n",
    "    deltas = features_pad[2 * D:] - features_pad[0 : -2 * D]\n",
    "    sdc = deltas[:N]\n",
    "    # append the delta vector at distace k * P (k = 1, ..., K - 1)\n",
    "    for k in range(1, K):\n",
    "        sdc = np.concatenate((sdc, deltas[k * P: k * P + N]), axis = 1)\n",
    "        \n",
    "    if keep_static:\n",
    "        return np.concatenate((features, sdc), axis = 1)\n",
    "    else:\n",
    "        return sdc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can think of several strattegies to compute VAD, simple ones based on energy and a threshold, or maybe some more \n",
    "# ellaborated ones, like training a GMM with 2 mixtures with the Energy. \n",
    "# In addition to the features without some frames, this function may return a sequence of 0s and 1s that helps you to validate the method.\n",
    "def compute_vad(features, energy, y=None):\n",
    "    gm = GaussianMixture(n_components = 2, random_state = GLOBAL_SEED).fit(energy)\n",
    "    voiced = np.argmax(gm.means_)\n",
    "    vad = gm.fit_predict(energy)\n",
    "    voiced_features = features[vad == voiced]\n",
    "    return voiced_features, vad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test in an isolated audio file and inspect the dimensions, verify that your code is doing what is expected, inspect and visualize the data using some of the lessons learnt in LAB1. Also, don't forget to listen some of the examples!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfcc_sdc, _ = feat_extract('sample.wav', apply_cmvn=False, apply_sdc=True)\n",
    "#mfcc_dd, _ = feat_extract('sample.wav', apply_cmvn=False, delta_order=2)\n",
    "#mfcc_d, _ = feat_extract('sample.wav', apply_cmvn=False, delta_order=1)\n",
    "#mfcc, y = feat_extract('sample.wav', apply_vad=True, num_formants = 5, cluster_order = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the data processing stage for the train100 partition we will simply instanciate the Kalaka class as mentioned previosly. Take a sit because it can take a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duartealmeida/PF/Spoken-Language-Processing/lab02/pf_tools.py:91: UserWarning: The feature directory already exists, and no new feature extraction will be performed.\n",
      "  warnings.warn(\"The feature directory already exists, and no new feature extraction will be performed.\")\n"
     ]
    }
   ],
   "source": [
    "sr = 16000\n",
    "sec = 8\n",
    "hop_length = 160\n",
    "vectors_4_sec = int((sr * sec) // hop_length)\n",
    "\n",
    "transform = { 'mfcc_sdc_vad_chunk_300_300' : \n",
    "                 { \n",
    "                     'audio_transform': lambda x : feat_extract(x, orig_sr=16000, mono=True, n_mfcc = 7, apply_sdc=True, apply_vad=True, apply_cmvn=True)[0],\n",
    "                     'chunk_transform': None,\n",
    "                     'chunk_size': 300,\n",
    "                     'chunk_hop':300\n",
    "                 },\n",
    "                 'mfcc13' : \n",
    "                 { \n",
    "                     'audio_transform': lambda x : feat_extract(x, orig_sr=16000, mono=True, n_mfcc = 13, apply_sdc=False, apply_vad=False, apply_cmvn=True)[0],\n",
    "                     'chunk_transform': None,\n",
    "                     'chunk_size': 300,\n",
    "                     'chunk_hop':300\n",
    "                 },\n",
    "                 'mfcc7' : \n",
    "                 { \n",
    "                     'audio_transform': lambda x: feat_extract(x, orig_sr=16000, mono=True, n_mfcc = 7, apply_sdc = False, apply_vad = True, apply_cmvn = True)[0],\n",
    "                     'chunk_transform': None,\n",
    "                     'chunk_size': vectors_4_sec,\n",
    "                     'chunk_hop': vectors_4_sec\n",
    "                 },\n",
    "                 'mfcc7_delta_deltadelta' : \n",
    "                 { \n",
    "                     'audio_transform': lambda x: feat_extract(x, orig_sr=16000, mono=True, n_mfcc = 7, apply_sdc = False, delta_order = 2, apply_vad = True, apply_cmvn = True)[0],\n",
    "                     'chunk_transform': None,\n",
    "                     'chunk_size': vectors_4_sec,\n",
    "                     'chunk_hop': vectors_4_sec\n",
    "                 },\n",
    "                 'mfcc4_formants5_cluster' : \n",
    "                 { \n",
    "                     'audio_transform': lambda x: feat_extract(x, orig_sr=16000, mono=True, n_mfcc = 4, apply_sdc = False, apply_vad = True, apply_cmvn = True, num_formants = 5)[0],\n",
    "                     'chunk_transform': None,\n",
    "                     'chunk_size': 300, \n",
    "                     'chunk_hop': 300, \n",
    "                 },\n",
    "            }\n",
    "\n",
    "\n",
    "trainset = 'train100'\n",
    "transform_id = 'mfcc_sdc_vad_chunk_300_300'\n",
    "\n",
    "trainkalaka = Kalaka(CWD, trainset, \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Check your current folder, many things happened!! \n",
    "\n",
    "Notice that if you instanciate again the Kalaka class for the 'train100' partition, the data will not be downloaded again. \n",
    "Additionally, if there is already a folder with the name `transform_id`, feature extraction will not run again. You need to delete from your filesystem the folder with the features if you want to run again the feature extraction (using the same identifier) or , alternatively, you can change the identifier. Be careful because you can easily increase the amount of data generated. If you try a feature extraction method that provides bad results, you probably don't want to keep the features in disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The baseline spoken language models\n",
    "The baseline model is extremely simple: we'll train an individual GMM model for each language on top of the features that we just extracted. Later, in prediction time, given a test audio sample, we'll compute the loglikelihood obtained with each GMM model and select as the identified language the one whose model gives the highest likelihood. Let's go for it!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1667100, 55)\n",
      "(1667100,)\n",
      "Finished reading all data in 5.030212879180908\n"
     ]
    }
   ],
   "source": [
    "# IN GMM training each training sample contains more than one frame;\n",
    "# so we cocatenate all data to have all training datain one array \n",
    "# and the corresponding label with same time duration\n",
    "# actually, here the chunking process is useless, \n",
    "#I could have obtained the same without chunking (with slight differences due to trunkation)\n",
    "\n",
    "start = time.time()\n",
    "train_data = []\n",
    "train_labels = []\n",
    "dict_languages = {}\n",
    "for data, label, basename in trainkalaka:\n",
    "    train_data.append(data)\n",
    "    train_labels.append(np.full(data.shape[0], label)) \n",
    "    \n",
    "#train_data = np.concatenate(train_data)\n",
    "#train_labels = np.concatenate(train_labels)\n",
    "#train_basenames = np.concatenate(train_basenames)\n",
    "\n",
    "#new_train_data = []\n",
    "#new_train_labels = []\n",
    "#for basename in basenames:\n",
    "#    vectors = train_data[train_basenames == basename]\n",
    "#    label = train_labels[train_basenames == basename][0]\n",
    "#    vectors_tr = vectors[:len(vectors) - (len(vectors) % 1000)]\n",
    "#    new_vectors = vectors_tr.reshape((-1, 1000, vectors_tr.shape[1]))\n",
    "#    new_train_data.append(new_vectors)\n",
    "#    new_train_labels.append(np.full(new_vectors.shape[0], label - 1))\n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two arrays containing the complete training dataset and the corresponging reference labels. Check the sizes, may be have a look to the content of one time instant. Do some checks on the data to be sure that everything is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the traininig data. Notice that if you apply VAD, the size of the training data must be smaller than the complete data set. \n",
    "# Register the size in frames and in time of training data for each language\n",
    "#raise CheckThisCell ## <---- Remove this after completeing/checking this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go training. Again, depending of the amount of data used, the model complexity and computational resources of the machine that you're using, this can take a while. So, relax while the computer works for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1667100, 55)\n",
      "Training model for Basque\n",
      "Initialization 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lang \u001b[38;5;129;01min\u001b[39;00m LANGUAGES:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     models[lang]\u001b[38;5;241m.\u001b[39mfit(train_data[train_labels\u001b[38;5;241m==\u001b[39mLANG2ID[lang]])\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/mixture/_base.py:186\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_predict(X, y)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/mixture/_base.py:241\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_verbose_msg_init_beg(init)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_init:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_parameters(X, random_state)\n\u001b[1;32m    243\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf \u001b[38;5;28;01mif\u001b[39;00m do_init \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower_bound_\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/mixture/_base.py:119\u001b[0m, in \u001b[0;36mBaseMixture._initialize_parameters\u001b[0;34m(self, X, random_state)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_params \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkmeans\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    114\u001b[0m     resp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components))\n\u001b[1;32m    115\u001b[0m     label \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    116\u001b[0m         cluster\u001b[38;5;241m.\u001b[39mKMeans(\n\u001b[1;32m    117\u001b[0m             n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mrandom_state\n\u001b[1;32m    118\u001b[0m         )\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m     resp[np\u001b[38;5;241m.\u001b[39marange(n_samples), label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_params \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1461\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1457\u001b[0m best_inertia, best_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_init):\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;66;03m# Initialize centers\u001b[39;00m\n\u001b[0;32m-> 1461\u001b[0m     centers_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_centroids(\n\u001b[1;32m   1462\u001b[0m         X, x_squared_norms\u001b[38;5;241m=\u001b[39mx_squared_norms, init\u001b[38;5;241m=\u001b[39minit, random_state\u001b[38;5;241m=\u001b[39mrandom_state\n\u001b[1;32m   1463\u001b[0m     )\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m   1465\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:989\u001b[0m, in \u001b[0;36m_BaseKMeans._init_centroids\u001b[0;34m(self, X, x_squared_norms, init, random_state, init_size, n_centroids)\u001b[0m\n\u001b[1;32m    986\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-means++\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 989\u001b[0m     centers, _ \u001b[38;5;241m=\u001b[39m _kmeans_plusplus(\n\u001b[1;32m    990\u001b[0m         X,\n\u001b[1;32m    991\u001b[0m         n_clusters,\n\u001b[1;32m    992\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[1;32m    993\u001b[0m         x_squared_norms\u001b[38;5;241m=\u001b[39mx_squared_norms,\n\u001b[1;32m    994\u001b[0m     )\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    996\u001b[0m     seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mpermutation(n_samples)[:n_clusters]\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:234\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[0;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m    231\u001b[0m np\u001b[38;5;241m.\u001b[39mclip(candidate_ids, \u001b[38;5;28;01mNone\u001b[39;00m, closest_dist_sq\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m=\u001b[39mcandidate_ids)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Compute distances to center candidates\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m distance_to_candidates \u001b[38;5;241m=\u001b[39m _euclidean_distances(\n\u001b[1;32m    235\u001b[0m     X[candidate_ids], X, Y_norm_squared\u001b[38;5;241m=\u001b[39mx_squared_norms, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[1;32m    239\u001b[0m np\u001b[38;5;241m.\u001b[39mminimum(closest_dist_sq, distance_to_candidates, out\u001b[38;5;241m=\u001b[39mdistance_to_candidates)\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:366\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    361\u001b[0m         YY \u001b[38;5;241m=\u001b[39m row_norms(Y, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# To minimize precision issues with float32, we compute the distance\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# matrix on chunks of X and Y upcast to float64\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     distances \u001b[38;5;241m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m safe_sparse_dot(X, Y\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:568\u001b[0m, in \u001b[0;36m_euclidean_distances_upcast\u001b[0;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     YY_chunk \u001b[38;5;241m=\u001b[39m YY[:, y_slice]\n\u001b[0;32m--> 568\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m safe_sparse_dot(X_chunk, Y_chunk\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    569\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX_chunk\n\u001b[1;32m    570\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY_chunk\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/sklearn/utils/extmath.py:192\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 192\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m ):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/PFEnv/lib/python3.11/site-packages/scipy/sparse/_base.py:1301\u001b[0m, in \u001b[0;36misspmatrix\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m-> 1301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misspmatrix\u001b[39m(x):\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is x of a sparse matrix type?\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \n\u001b[1;32m   1304\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, spmatrix)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN GMM models (ML) \n",
    "\n",
    "print(train_data.shape)\n",
    "models = {}\n",
    "n_gauss = 256\n",
    "for lang in LANGUAGES:\n",
    "    models[lang] = GaussianMixture(\n",
    "                        n_components=n_gauss, \n",
    "                        covariance_type='diag', \n",
    "                        max_iter=20, n_init=1, \n",
    "                        init_params='kmeans', \n",
    "                        verbose=2, \n",
    "                        verbose_interval=1)\n",
    "    #models[\"not_\" + lang] = GaussianMixture(\n",
    "    #                    n_components=n_gauss, \n",
    "    #                    covariance_type='diag', \n",
    "    #                    max_iter=20, n_init=1, \n",
    "    #                    init_params='kmeans', \n",
    "    #                    verbose=2, \n",
    "    #                    verbose_interval=1)\n",
    "    \n",
    "for lang in LANGUAGES:\n",
    "    print(f'Training model for {lang}')\n",
    "    models[lang].fit(train_data[train_labels==LANG2ID[lang]])\n",
    "    \n",
    "    #print(f'Training model for not {lang}')\n",
    "    #models[\"not_\" + lang].fit(train_data[train_labels!=LANG2ID[lang]])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models have been trained, we can store them in disk for later usage. Again, be careful and avoid storing versions of useless models. By default, the model is stored in a folder inside the data partition folder and contains the feature extraction in the name and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models     \n",
    "now = str(datetime.datetime.now()).replace(' ','_').split('.')[0]\n",
    "path = Path(CWD) / trainset / 'models'\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "filename = f'{path}/gmm_256_{transform_id}_{now}.model'\n",
    "pickle.dump(models, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the `sklearn` documentation and inspect the models trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Portuguese'].means_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification and generation of the predictions file\n",
    "\n",
    "Now that we  already have trained models, let's predict/identify language in new audio data and test our model!!! \n",
    "\n",
    "But first, we need to obtain the development partition and do the feature extraction more or less as previously (using the Kalaka class).\n",
    "\n",
    "**IMPORTANT WARNING** Make sure to use the exact same feature extraction process as the one used for the train set. Otherwise, your model will be in disagreement with your evaluation data, and very likely, will not work at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and feature extract\n",
    "transform_id = 'mfcc_sdc_vad_chunk_300_300'\n",
    "#transform_id = 'mfcc4_formants5_cluster'\n",
    "\n",
    "# Notice that the chunking is mostly useless both in training and prediction with GMMs: each frame is a sample for which we obtain the probs\n",
    "devkalaka = Kalaka(CWD,'dev', \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can iterate the data and use the models for scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dev_data = {}\n",
    "\n",
    "\n",
    "for data, label, basename in devkalaka:\n",
    "        if basename not in dev_data:\n",
    "                dev_data[basename] = {'data':[], 'label':label}\n",
    "        dev_data[basename]['data'].append(data)\n",
    "\n",
    "## We concatenate all the frames belonging to the same filename\n",
    "for basename in dev_data:\n",
    "        dev_data[basename]['data'] = np.concatenate(dev_data[basename]['data'])\n",
    "print(f'Finished reading all data in {time.time() - start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "results_dev = {}\n",
    "results_dev['ref'] =  np.empty(len(dev_data),dtype=np.int32)\n",
    "results_dev['hyp'] =  np.empty(len(dev_data),dtype=np.int32)\n",
    "results_dev['llhs'] = np.empty((len(dev_data), len(LANGUAGES)), dtype=np.float64)\n",
    "results_dev['fileids'] = list()\n",
    "\n",
    "i = 0\n",
    "for i, fileid in tqdm(enumerate(sorted(dev_data)), total=len(dev_data)):\n",
    "    data = dev_data[fileid][\"data\"]\n",
    "    \n",
    "    results_dev['fileids'].append(fileid)     #fileid\n",
    "    \n",
    "    # obtain the log-likelihood score for each model and store\n",
    "    results_dev['llhs'][i,:] = np.array([models[lang].score(data) for lang in LANGUAGES]) \n",
    "    # store the reference. Notice that I only have this for the dev set, not for the eval\n",
    "    results_dev['ref'][i] = (dev_data[fileid]['label']) #referemce\n",
    "\n",
    "    # Obtain the maximum likelihood languge estimation\n",
    "    ix = np.argmax(results_dev['llhs'][i,:])\n",
    "    results_dev['hyp'][i] = LANG2ID[LANGUAGES[ix]]\n",
    "\n",
    "print(f'Finished predicting all data in {time.time() - start}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation\n",
    "After running the previous cells, we obtain two arrays with the reference and hypothesis labels, respectively (we can also reload them in case we need them). We can use these to obtain different evaluation metrics and inspect the performance (and potential problems) of our system. Of course, you will only be able to do this evaluation with the development set, since you don't have access to the eval labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can for instance obtain a classification report summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref, hyp = results_dev['ref'], results_dev['hyp']\n",
    "print(classification_report(ref, hyp, target_names=LANGUAGES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall accuracy (this will be the **main metric for system ranking**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ref, hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a confusion matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(ref, hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(ref, hyp)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.0, 3.0))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=12)\n",
    "plt.ylabel('Actuals', fontsize=12)\n",
    "plt.title(f'Confusion Matrix\\n(Accuracy {100*accuracy_score(ref, hyp):.2f})', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are poor. The baseline system is very limited in several aspects (features, time context, generative model, etc.). For instance, the likelihood scores are not normalized. It may happen, that one model  provides slightly higher scores for some reason. The following trick sometimes increases slightly the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llhs_norm = (results_dev['llhs'] - results_dev['llhs'].mean(axis=0))\n",
    "hyp_norm = np.empty(hyp.shape, like=hyp)\n",
    "for i in range(len(results_dev['fileids'])):\n",
    "    ix = np.argmax(llhs_norm[i,:])\n",
    "    hyp_norm[i] = LANG2ID[LANGUAGES[ix]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(ref, hyp_norm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.0, 3.0))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=12)\n",
    "plt.ylabel('Actuals', fontsize=12)\n",
    "plt.title(f'Confusion Matrix\\n(Accuracy {100*accuracy_score(ref, hyp_norm):.2f})', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"trick\" is probematic. Can you discuss why? Can you think of an altenative way of doing the same kind of normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the prediction on the evl partition\n",
    "\n",
    "Once you are happy with your system and the results obtained in the development set, you are ready to generate the predictions on the `'evl'` partition. To do that, you have to follow the same process as for the development partition, but of course, this time you will not be able to obtain performance results because you don't have labels for this partition. \n",
    "\n",
    "We start by instantiating the `Kalaka` class for the `'evl'` partition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_id = 'mfcc_sdc_vad_chunk_300_300'\n",
    "\n",
    "evlkalaka = Kalaka(CWD,'evl', \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "evl_data = {}\n",
    "\n",
    "for data, label, basename in evlkalaka:\n",
    "        if basename not in evl_data:\n",
    "                evl_data[basename] = {'data':[], 'label':label}\n",
    "        evl_data[basename]['data'].append(data)\n",
    "\n",
    "for basename in evl_data:\n",
    "        evl_data[basename]['data'] = np.concatenate(evl_data[basename]['data'])\n",
    "\n",
    "print(f'Finished reading all data in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply the model(s) to the new `'evl'` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "results_evl = {}\n",
    "results_evl['ref'] =  None\n",
    "results_evl['hyp'] =  np.empty(len(evl_data),dtype=np.int32)\n",
    "results_evl['llhs'] = np.empty((len(evl_data), len(LANGUAGES)), dtype=np.float64)\n",
    "results_evl['fileids'] = list()\n",
    "\n",
    "\n",
    "# Obtain LLH matrix\n",
    "for i, fileid in tqdm(enumerate(sorted(evl_data)), total=len(evl_data)):\n",
    "\n",
    "    data = evl_data[fileid]['data']  # the features\n",
    "    results_evl['fileids'].append(fileid)     #fileid\n",
    "\n",
    "    # obtain the log-likelihood score for each model and store\n",
    "    results_evl['llhs'][i,:] = np.array([models[lang].score(data) for lang in LANGUAGES])\n",
    "\n",
    "    # Obtain the maximum likelihood languge estimation\n",
    "    ix = np.argmax(results_evl['llhs'][i,:])\n",
    "    results_evl['hyp'][i] = LANG2ID[LANGUAGES[ix]]    \n",
    "\n",
    "print(f'Finished predicting all data in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the predictions file\n",
    "\n",
    "The predictions file used for submission and scoring is a CSV file containing the predictions of both the `dev` and `evl` partitions.\n",
    "The file has two fields: fileId and Lang. The fileId is the unique audio file identifier and the Lang field is the language prediction (numeric from 1 to 6). The predictions file name must be as follows:\n",
    "\n",
    "`T<X>_G<YY>_<SYSTEMID>.csv` \n",
    "\n",
    "where `<X>` can be 1 or 2 depending on being a system for track 1 or track 2 evaluation; `<YY>` is the students' group number (use 2 digits) and `<SYSTEMID>` is a identifying string for that submission/system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group, system = '00', 'baseline_train100'\n",
    "with open(f'{os.getcwd()}/T1_G{group}_{system}.csv', 'w') as file:\n",
    "    csv_writer = csv.writer(file) # CSV writer\n",
    "    csv_writer.writerow(('fileId', 'Lang')) # Header of the CSV\n",
    "\n",
    "    # Save dev results\n",
    "    for i in range(len(results_dev['fileids'])):\n",
    "        csv_writer.writerow((results_dev['fileids'][i], results_dev['hyp'][i]))\n",
    "    # Save evl results\n",
    "    for i in range(len(results_evl['fileids'])):\n",
    "        csv_writer.writerow((results_evl['fileids'][i], results_evl['hyp'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your prediction\n",
    "You can submit your prediction in the following Kaggle competition: https://www.kaggle.com/competitions/speech-processing-lab-2/\n",
    "\n",
    "## What should/can you do next?\n",
    "**Everything!!** Try to extend and improve the feature extraction. Try to play with the parameters. Try completely different feature extraction modules (look for openSMILE, torchaudio,). Try to increase the model complexity. Try different modeling approaches. Try to understand the impact of the different modifications. Once you are happy with your system, try to train on the full set and check the impact of adding data to your system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - Using pre-trained embeddings (Track 2)\n",
    "\n",
    "There exist plenty of resources and pre-trained models that can be extremely useful for our task. For instance, x-vectors are currently the state of the art approach to obtain speech embeddings that characterize very efficiently speaker or language, among others. Particularly, the following x-vector model is available and it has been trained using a large corpus of 107 languages for language identification: https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa\n",
    "\n",
    "You can obtain it from the `speechbrain` module, that you need to install now if you are using Google Colab:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Google Colab, you'll have to install the speechbrain module\n",
    "#raise CheckThisCell ## <---- Remove this after completing/checking this cell\n",
    "#!pip install speechbrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell imports such model and shows how to obtain an embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "/Users/duartealmeida/miniconda3/envs/PFEnv/lib/python3.11/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/SpectralOps.cpp:867.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8070e+01,  4.9016e+01, -1.1503e+01,  3.4845e+01,  2.9686e+01,\n",
      "           1.2965e+01,  3.3693e+01, -7.0847e+00,  2.7003e+01,  1.3238e+00,\n",
      "           2.0937e+00,  4.6716e+01,  4.0381e+01,  2.7669e+01,  1.0301e+01,\n",
      "           3.5585e+01,  2.2612e+01,  1.2936e+00, -1.1567e+01, -1.3776e+01,\n",
      "          -1.1318e+01,  3.5121e+01,  4.2536e+00,  2.4499e+01,  3.0473e+01,\n",
      "          -5.9533e+00,  4.5652e+01,  3.7024e+01,  2.7054e+01, -8.8062e+00,\n",
      "          -8.1440e+00,  6.4745e+00, -2.6677e+01,  3.3796e+01, -3.0038e+01,\n",
      "          -1.7247e-01,  5.4481e+01,  3.9470e+01,  2.0512e+01, -8.1592e-02,\n",
      "           2.6525e+01,  3.1203e+01,  1.0836e+01,  3.2499e+01,  4.0244e+01,\n",
      "           2.0815e+01, -1.9671e+01,  2.0470e+01,  2.8246e+01,  1.0630e+01,\n",
      "           3.8383e+01,  3.9727e+01,  1.4472e+01, -1.2266e+01,  4.2925e+01,\n",
      "           2.0044e+01,  6.4850e+00,  3.2841e+01, -1.4509e+01, -1.2985e+01,\n",
      "           3.3707e+01, -9.6947e-01,  1.0855e+01,  6.0152e+01,  2.4352e+01,\n",
      "           2.1603e+01, -9.7926e+00,  2.9390e+01, -2.2173e+00, -3.3638e+00,\n",
      "           4.8273e+01,  1.0692e+01,  3.8967e+01,  2.8704e+01, -5.8261e+00,\n",
      "           3.2022e+01,  2.4757e+01, -7.7966e+00,  1.8626e+01, -7.8224e+00,\n",
      "           2.8190e+01,  4.9842e+00,  4.2994e+00, -8.0592e-01,  2.9977e+01,\n",
      "           4.1527e+01, -1.3047e+00, -1.2731e+01,  1.9631e+01,  2.8731e+00,\n",
      "           1.8824e+01,  5.0482e+01,  2.2286e+01, -1.0218e+00,  4.6919e+01,\n",
      "           2.1899e+01,  2.4317e+01,  2.6492e+01,  3.3278e+01,  3.2031e+01,\n",
      "          -4.1231e+01,  3.4662e+01,  2.4520e+01,  2.8429e+01,  3.5341e+01,\n",
      "          -1.0551e+01,  4.3213e+01,  1.5197e+01,  1.7396e+01,  7.4938e+00,\n",
      "          -2.3048e-02,  4.3253e+01,  5.4199e+01,  2.3842e+01,  5.2045e+01,\n",
      "           3.1460e+01, -1.5323e+01,  3.2432e+01,  3.5282e+01,  1.0007e+01,\n",
      "          -8.0838e+00,  4.3653e+01,  4.1021e+01,  2.4784e+01, -4.7729e+00,\n",
      "           4.7695e+01,  2.3475e+01,  1.1837e+01, -1.6561e+01, -8.0444e+00,\n",
      "           4.1422e+01,  4.2183e+00,  1.5965e+01,  2.0727e+01, -7.9551e+00,\n",
      "           7.9424e+00,  1.5093e+01, -2.0001e+01,  3.6952e+00,  1.0057e+01,\n",
      "           4.0957e+01,  2.7816e+01,  3.3992e+01,  3.3837e+01,  1.2656e+01,\n",
      "           4.1203e+01,  3.5218e+00,  2.7509e+01, -2.2015e+00,  2.2821e+01,\n",
      "           2.9692e+00,  2.0169e+01,  2.6165e+01,  1.6486e+00,  2.5613e+01,\n",
      "           2.4129e+01,  1.8668e+01,  1.5845e+01,  3.5413e+01, -6.7563e-01,\n",
      "           3.6949e+01, -3.6415e+00, -1.0221e+01, -1.7361e+01, -1.4424e+01,\n",
      "           2.9268e+01,  2.5267e+01,  3.5836e+01,  6.0360e+00,  2.5561e+01,\n",
      "           2.9744e+01, -2.3328e+00, -5.0860e+00, -1.9902e+01, -1.3084e+01,\n",
      "           1.5151e+01, -4.2092e-01,  4.0266e+01, -2.8743e+00, -2.6095e+00,\n",
      "           3.4798e+01,  4.2029e+01,  2.4344e+00, -2.8125e+00,  3.1801e+01,\n",
      "          -2.0691e+01,  4.1834e-01,  2.1168e+01,  3.7587e+01,  5.5274e+00,\n",
      "           1.5494e+01,  2.0039e+01, -1.5568e+01,  1.8700e+00,  4.4543e+01,\n",
      "          -1.5364e+01,  4.0498e+01, -2.1164e+01,  2.2235e+01,  2.0061e+01,\n",
      "           1.9529e+01,  2.8505e+01, -1.0544e+01, -1.1293e+01, -1.0593e+01,\n",
      "           1.6494e+01,  2.7672e+01,  4.7490e+01,  2.9543e+01,  4.2975e+01,\n",
      "           1.1277e+01,  5.4494e-01,  3.3305e+01,  1.9006e-01,  1.8360e+00,\n",
      "          -1.2950e+01,  3.8245e+01,  2.1754e+00,  4.8622e+01, -1.9358e+01,\n",
      "           1.7756e+01, -1.1256e+01,  2.6004e+01,  1.1220e+01, -1.8738e+00,\n",
      "           4.3551e+01,  1.9475e+00,  2.7118e+01,  2.7920e+01, -1.0213e+01,\n",
      "           1.1461e+01,  3.4331e+01,  2.0447e+01, -3.8040e+00,  4.5237e+01,\n",
      "           1.6698e+01,  8.7658e+00, -3.7491e+01,  4.7739e+01,  1.4744e+01,\n",
      "           2.4159e+01,  2.9657e+01,  3.9794e+01,  4.6539e+01,  2.2086e+01,\n",
      "           4.0285e+01, -7.6225e+00, -2.5155e+01, -2.2113e+01, -1.0897e+01,\n",
      "           3.7085e+01,  9.5355e+00,  1.4110e+01,  7.7991e+00, -3.7041e+00,\n",
      "           3.4251e+01]]])\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import speechbrain\n",
    "\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "language_id = EncoderClassifier.from_hparams(source=\"speechbrain/lang-id-voxlingua107-ecapa\", savedir=\"tmp\")\n",
    "\n",
    "signal = language_id.load_audio(f'{CWD}/train100/audio/0a5c0729.wav')\n",
    "emb =  language_id.encode_batch(signal)\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the model is trained for language identification of 107 languages and we could use it directly for identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction =  language_id.classify_batch(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invest some time to inspect the model and the outputs. Notice that the six target languages are included among the 107. The output indices are: \n",
    "\n",
    "```python\n",
    "XVEC_LANG_INDEX = (24,13,20,29,75,22) \n",
    "```\n",
    "\n",
    "corresponding respectively to the following languages:\n",
    "\n",
    "```python \n",
    "LANGUAGES = ('Basque',  'Catalan',  'English',  'Galician',  'Portuguese',  'Spanish')\n",
    "```\n",
    "\n",
    "Knowing this, it should be easy to obtain the predicted class among the six possible candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XVEC_LANG_INDEX = (24,13,20,29,75,22) \n",
    "\n",
    "# Obtain the predicted class out of the 6 target ones\n",
    "print(prediction)\n",
    "#res = prediction[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the x-vector up-stream model for language ID\n",
    "\n",
    "Let's try first something simple: use the the pre-trained model for idenfication. In this simple test, you don't need to train anyhing simply classify the dev and test sets. To do so, first configure the right transformation for the Kalaka class (one that simply loads the audio without any chunking):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xvec(x):\n",
    "    signal = librosa.load(x, sr=16000, mono=True)[0]\n",
    "    emb =  language_id.encode_batch(torch.tensor(signal))\n",
    "    return emb[0][0]\n",
    "def extract_xvec_segments(x, sec):\n",
    "  signal = torch.tensor(librosa.load(x, sr=16000, mono=True)[0])\n",
    "  split_signal = torch.split(signal, (16000 * sec))\n",
    "  batch_split = pad_sequence(split_signal, batch_first=True, padding_value=0)\n",
    "  emb = language_id.encode_batch(batch_split)\n",
    "  return emb.reshape(emb.shape[0], emb.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 256)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "trainset = 'train100'\n",
    "transform_id = 'raw_xvec'\n",
    "\n",
    "transform = { \n",
    "                'raw_xvec' :\n",
    "                {\n",
    "                    'audio_transform': lambda x: extract_xvec(x),\n",
    "                    'chunk_transform': None,\n",
    "                    'chunk_size': 0,\n",
    "                    'chunk_hop':0   \n",
    "                },\n",
    "                'raw_xvec_segments' :\n",
    "                {\n",
    "                    'audio_transform': lambda x: extract_xvec_segments(x, 2),\n",
    "                    'chunk_transform': None,\n",
    "                    'chunk_size': 0,\n",
    "                    'chunk_hop':0   \n",
    "                }\n",
    "            }\n",
    "\n",
    "trainkalaka = Kalaka(CWD, trainset, \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "dict_languages = {}\n",
    "for data, label, basename in trainkalaka:\n",
    "    train_data.append(data.numpy())\n",
    "    train_labels.append(label)\n",
    "    #train_data.append(data.numpy())\n",
    "    #train_labels.append(np.full(data.shape[0], label)) \n",
    "    \n",
    "#train_data = np.concatenate(train_data)\n",
    "#train_labels = np.concatenate(train_labels)\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]*\n",
      "optimization finished, #iter = 69\n",
      "obj = -32.792319, rho = 0.421127\n",
      "nSV = 64, nBSV = 48\n",
      "*\n",
      "optimization finished, #iter = 30\n",
      "obj = -9.150235, rho = 0.747600\n",
      "nSV = 22, nBSV = 11\n",
      "*\n",
      "optimization finished, #iter = 85\n",
      "obj = -40.103359, rho = -0.319468\n",
      "nSV = 81, nBSV = 62\n",
      "*\n",
      "optimization finished, #iter = 47\n",
      "obj = -17.546929, rho = 0.535565\n",
      "nSV = 39, nBSV = 27\n",
      "*\n",
      "optimization finished, #iter = 92\n",
      "obj = -52.596153, rho = 0.518860\n",
      "nSV = 97, nBSV = 81\n",
      "*\n",
      "optimization finished, #iter = 35\n",
      "obj = -10.897608, rho = 0.753683\n",
      "nSV = 24, nBSV = 12\n",
      "*\n",
      "optimization finished, #iter = 71\n",
      "obj = -38.324421, rho = -0.712699\n",
      "nSV = 73, nBSV = 59\n",
      "*\n",
      "optimization finished, #iter = 45\n",
      "obj = -25.699161, rho = 0.265842\n",
      "nSV = 50, nBSV = 41\n",
      "*\n",
      "optimization finished, #iter = 79\n",
      "obj = -50.016696, rho = -0.196234\n",
      "nSV = 92, nBSV = 76\n",
      "*\n",
      "optimization finished, #iter = 35\n",
      "obj = -7.655351, rho = -0.980745\n",
      "nSV = 19, nBSV = 9\n",
      "*\n",
      "optimization finished, #iter = 43\n",
      "obj = -13.252981, rho = -0.859213\n",
      "nSV = 29, nBSV = 17\n",
      "*\n",
      "optimization finished, #iter = 34\n",
      "obj = -8.035999, rho = -0.898831\n",
      "nSV = 20, nBSV = 10\n",
      "*\n",
      "optimization finished, #iter = 48\n",
      "obj = -18.602884, rho = 0.841350\n",
      "nSV = 39, nBSV = 29\n",
      "*\n",
      "optimization finished, #iter = 116\n",
      "obj = -97.682143, rho = 1.442583\n",
      "nSV = 150, nBSV = 133\n",
      "*\n",
      "optimization finished, #iter = 35\n",
      "obj = -16.999454, rho = -0.462165\n",
      "nSV = 35, nBSV = 27\n",
      "Total nSV = 350\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(degree=1, kernel=&#x27;poly&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(degree=1, kernel=&#x27;poly&#x27;, verbose=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(degree=1, kernel='poly', verbose=2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pca = PCA()\n",
    "pca.fit(train_data)\n",
    "cumulative_var_ratios = np.cumsum(pca.explained_variance_ratio_)\n",
    "order = np.argwhere(cumulative_var_ratios > 0.95)[0][0]\n",
    "\n",
    "pca_model = PCA(n_components = order + 1)\n",
    "pca_model.fit(train_data)\n",
    "components = pca_model.components_\n",
    "\n",
    "train_data_reduced = train_data @ components.T\n",
    "print(train_data_reduced.shape)\n",
    "'''\n",
    "\n",
    "model = SVC(kernel = \"linear\", verbose = 2)\n",
    "model.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duartealmeida/PF/Spoken-Language-Processing/lab02/pf_tools.py:91: UserWarning: The feature directory already exists, and no new feature extraction will be performed.\n",
      "  warnings.warn(\"The feature directory already exists, and no new feature extraction will be performed.\")\n"
     ]
    }
   ],
   "source": [
    "# Confifure the tranformation\n",
    "#raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "\n",
    "\n",
    "# Download and feature extract\n",
    "transform_id = 'raw_xvec'\n",
    "\n",
    "\n",
    "devkalaka = Kalaka(CWD, 'dev', \n",
    "                 transform_id=transform_id, \n",
    "                 audio_transform=transform[transform_id]['audio_transform'], \n",
    "                 chunk_transform=transform[transform_id]['chunk_transform'],\n",
    "                 chunk_size=transform[transform_id]['chunk_size'], \n",
    "                 chunk_hop=transform[transform_id]['chunk_hop']\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have all the elements to predict langauge using the langID pre-trained model.\n",
    "In case you have access to a GPU, it may be good using a Pytorch Dataloader to batch the dev samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the audios are of different size, we need to pass an auxiliary function to the dataloader that handles this (by adding 0s). This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch): \n",
    "    label_list, audio_list, basename_list, audiolen_list = [], [], [], []\n",
    "\n",
    "    for (_audio,_label, _basename) in batch:\n",
    "        label_list.append(_label)\n",
    "        audio_list.append(_audio)\n",
    "        basename_list.append(_basename)\n",
    "        audiolen_list.append(_audio.shape[0])\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    audio_list = pad_sequence(audio_list, batch_first=True, padding_value=0)\n",
    "    max_len = max(audiolen_list)\n",
    "    audiolen_list = torch.tensor([l/max_len for l in audiolen_list])\n",
    "\n",
    "    return audio_list.to(device),label_list.to(device), basename_list, audiolen_list.to(device)\n",
    "\n",
    "batch_size = 1\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=devkalaka,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_batch,\n",
    "        shuffle=False  # <-- We want to keep the original order of the dev set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following piece of code to store properly the hypothesis and the reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████▉| 917/918 [00:00<00:00, 1328.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preocessing all data in 0.69439697265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "start = time.time()\n",
    "\n",
    "results_dev = {}\n",
    "results_dev['ref'] =  []\n",
    "results_dev['hyp'] =  []\n",
    "results_dev['fileids'] = [] \n",
    "\n",
    "for i, batch in tqdm(enumerate(dataloader), total=1+len(devkalaka)//batch_size):\n",
    "    data, label, basename, audiolen = batch\n",
    "    #predictions = language_id.classify_batch(data, wav_lens=audiolen)\n",
    "    predictions = model.predict(data)\n",
    "    \n",
    "    # Complete the code to store the hypothesis (careful if you use argmax, you will ned to add 1 to the predicted class),\n",
    "    # the reference and the fileids. In the call to the prediction method, you should pass the audio length information, so that \n",
    "    # the padded 0s can be ignored\n",
    "    results_dev[\"hyp\"].append(predictions[0])\n",
    "    results_dev[\"ref\"].append(label[0])\n",
    "    results_dev[\"fileids\"].append(basename)\n",
    "    #raise CheckThisCell ## <---- Remove this after completeing/checking this cell\n",
    "    \n",
    "print(f'Finished preocessing all data in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's see how well this model behaves on our dev partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ref, hyp = results_dev['ref'], results_dev['hyp']\n",
    "\n",
    "conf_matrix = confusion_matrix(ref, hyp)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.0, 3.0))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=12)\n",
    "plt.ylabel('Actuals', fontsize=12)\n",
    "plt.title(f'Confusion Matrix\\n(Accuracy {100*accuracy_score(ref, hyp):.2f})', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was pretty impressive, right!?! And  easier than Part I of this lab! \n",
    "\n",
    "Well, here comes some bad news: you can not use this langid classifier in the challenge. However, you can use the pre-trained x-vector embeddings as a feature extractor to train your language classification system (may be a simple K-means on top of the x-vectors work well). You can also try to change the classification head of the x-vector model and fine-tune with the challenge data. You have plenty of options, but remember, any system that use these pre-trained embeddings (or any similar ones) are only valid for the Track 2 of the Challenge and the prediction file should be  namedlike this: `T2_G<YY>_<SYSTEMID>.csv` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What should you deliver at the end of this lab assignment?\n",
    "You should deliver the following three elements:\n",
    "- You must submit at least one prediction file to the Kaggle competition in the format previously described: https://www.kaggle.com/competitions/speech-processing-lab-2/\n",
    "- You must submit (via Fênix) the modified notebook (or code) of your proposed systems(s).\n",
    "- You must submit a report (via Fênix) of maximum 2 pages describing your system, approaches (may be unsuccesful), lessons learnt, results on the dev partition, etc. You can use the following Overleaf template for the report: https://www.overleaf.com/latex/templates/interspeech-2023-paper-kit/kzcdqdmkqvbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contacts and support\n",
    "You can contact the professors during the classes or the office hours.\n",
    "\n",
    "Particularly, for this second laboratory assignment, you should contact Prof. Alberto Abad: alberto.abad@tecnico.ulisboa.pt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
